@article{Barnett_2021_CaseBasedInterpretableDeep,
  title = {A Case-Based Interpretable Deep Learning Model for Classification of Mass Lesions in Digital Mammography},
  author = {Barnett, Alina Jade and Schwartz, Fides Regina and Tao, Chaofan and Chen, Chaofan and Ren, Yinhao and Lo, Joseph Y. and Rudin, Cynthia},
  year = {2021},
  month = dec,
  journal = {Nature Machine Intelligence},
  volume = {3},
  number = {12},
  pages = {1061--1070},
  publisher = {Nature Publishing Group},
  issn = {2522-5839},
  doi = {10.1038/s42256-021-00423-x},
  urldate = {2024-12-03},
  abstract = {Interpretability in machine learning models is important in high-stakes decisions such as whether to order a biopsy based on a mammographic exam. Mammography poses important challenges that are not present in other computer vision tasks: datasets are small, confounding information is present and it can be difficult even for a radiologist to decide between watchful waiting and biopsy based on a mammogram alone. In this work we present a framework for interpretable machine learning-based mammography. In addition to predicting whether a lesion is malignant or benign, our work aims to follow the reasoning processes of radiologists in detecting clinically relevant semantic features of each image, such as the characteristics of the mass margins. The framework includes a novel interpretable neural network algorithm that uses case-based reasoning for mammography. Our algorithm can incorporate a combination of data with whole image labelling and data with pixel-wise annotations, leading to better accuracy and interpretability even with a small number of images. Our interpretable models are able to highlight the classification-relevant parts of the image, whereas other methods highlight healthy tissue and confounding information. Our models are decision aids---rather than decision makers---and aim for better overall human--machine collaboration. We do not observe a loss in mass margin classification accuracy over a black box neural network trained on the same data.},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Breast cancer,Computational science,Computer science,Radiography}
}

@inproceedings{Bontempelli_2023_ConceptlevelDebuggingPartPrototype,
  title = {Concept-Level {{Debugging}} of {{Part-Prototype Networks}}},
  booktitle = {The {{Eleventh International Conference}} on {{Learning Representations}}},
  author = {Bontempelli, Andrea and Teso, Stefano and Tentori, Katya and Giunchiglia, Fausto and Passerini, Andrea},
  year = {2023},
  month = feb,
  urldate = {2024-12-03},
  abstract = {Part-prototype Networks (ProtoPNets) are concept-based classifiers designed to achieve the same performance as black-box models without compromising transparency. ProtoPNets compute predictions based on similarity to class-specific part-prototypes learned to recognize parts of training examples, making it easy to faithfully determine what examples are responsible for any target prediction and why. However, like other models, they are prone to picking up confounders and shortcuts from the data, thus suffering from compromised prediction accuracy and limited generalization. We propose ProtoPDebug, an effective concept-level debugger for ProtoPNets in which a human supervisor, guided by the model's explanations, supplies feedback in the form of what part-prototypes must be forgotten or kept, and the model is fine-tuned to align with this supervision. Our experimental evaluation shows that ProtoPDebug outperforms state-of-the-art debuggers for a fraction of the annotation cost. An online experiment with laypeople confirms the simplicity of the feedback requested to the users and the effectiveness of the collected feedback for learning confounder-free part-prototypes. ProtoPDebug is a promising tool for trustworthy interactive learning in critical applications, as suggested by a preliminary evaluation on a medical decision making task.},
  langid = {english},
  file = {/Users/khawla/Zotero/storage/4NC8UYBL/Bontempelli et al. - 2022 - Concept-level Debugging of Part-Prototype Networks.pdf}
}

@inproceedings{Cao_2021_ConceptLearnersFewShot,
  title = {Concept {{Learners}} for {{Few-Shot Learning}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Cao, Kaidi and Brbic, Maria and Leskovec, Jure},
  year = {2021},
  month = oct,
  urldate = {2024-12-03},
  abstract = {Developing algorithms that are able to generalize to a novel task given only a few labeled examples represents a fundamental challenge in closing the gap between machine- and human-level performance. The core of human cognition lies in the structured, reusable concepts that help us to rapidly adapt to new tasks and provide reasoning behind our decisions. However, existing meta-learning methods learn complex representations across prior labeled tasks without imposing any structure on the learned representations. Here we propose COMET, a meta-learning method that improves generalization ability by learning to learn along human-interpretable concept dimensions. Instead of learning a joint unstructured metric space, COMET learns mappings of high-level concepts into semi-structured metric spaces, and effectively combines the outputs of independent concept learners. We evaluate our model on few-shot tasks from diverse domains, including fine-grained image classification, document categorization and cell type annotation on a novel dataset from a biological domain developed in our work. COMET significantly outperforms strong meta-learning baselines, achieving 6-15\% relative improvement on the most challenging 1-shot learning tasks, while unlike existing methods providing interpretations behind the model's predictions.},
  langid = {english},
  file = {/Users/khawla/Zotero/storage/SIZIUHMV/Cao et al. - 2020 - Concept Learners for Few-Shot Learning.pdf}
}

@inproceedings{Carmichael_2024_ThisProbablyLooks,
  title = {This {{Probably Looks Exactly Like That}}: {{An Invertible Prototypical Network}}},
  shorttitle = {This {{Probably Looks Exactly Like That}}},
  author = {Carmichael, Zachariah and Redgrave, Timothy and Cedre, Daniel Gonzalez and Scheirer, Walter J.},
  year = {2024}, 
  isbn = {978-3-031-72912-6}, 
  publisher = {Springer-Verlag}, 
  address = {Berlin, Heidelberg}, 
  url = {https://doi.org/10.1007/978-3-031-72913-3_13}, 
  doi = {10.1007/978-3-031-72913-3_13}, 
  abstract = {We combine concept-based neural networks with generative, flow-based classifiers into a novel, intrinsically explainable, exactly invertible approach to supervised learning. Prototypical neural networks, a type of concept-based neural network, represent an exciting way forward in realizing human-comprehensible machine learning without concept annotations, but a human-machine semantic gap continues to haunt current approaches. We find that reliance on indirect interpretation functions for prototypical explanations imposes a severe limit on prototypes’ informative power. From this, we posit that invertibly learning prototypes as distributions over the latent space provides more robust, expressive, and interpretable modeling. We propose one such model, called ProtoFlow, by composing a normalizing flow with Gaussian mixture models. ProtoFlow (1) sets a new state-of-the-art in joint generative and predictive modeling and (2) achieves predictive performance comparable to existing prototypical neural networks while enabling richer interpretation.}, 
  booktitle = {Computer Vision – ECCV 2024: 18th European Conference, Milan, Italy, September 29–October 4, 2024, Proceedings, Part XXXVII}, 
  pages = {221–240}, 
  numpages = {20}, 
  keywords = {Normalizing flow, Prototypical neural networks, XAI}, 
  location = {Milan, Italy}
}

@inproceedings{Chen_2019_ThisLooksThat,
  title = {This {{Looks Like That}}: {{Deep Learning}} for {{Interpretable Image Recognition}}},
  shorttitle = {This {{Looks Like That}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Chen, Chaofan and Li, Oscar and Tao, Daniel and Barnett, Alina and Rudin, Cynthia and Su, Jonathan K},
  year = {2019},
  volume = {32},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-12-03},
  abstract = {When we are faced with challenging image classification tasks, we often explain our reasoning by dissecting the image, and pointing out prototypical aspects of one class or another. The mounting evidence for each of the classes helps us make our final decision. In this work, we introduce a deep network architecture -- prototypical part network (ProtoPNet), that reasons in a similar way: the network dissects the image by finding prototypical parts, and combines evidence from the prototypes to make a final classification. The model thus reasons in a way that is qualitatively similar to the way ornithologists, physicians, and others would explain to people on how to solve challenging image classification tasks. The network uses only image-level labels for training without any annotations for parts of images. We demonstrate our method on the CUB-200-2011 dataset and the Stanford Cars dataset. Our experiments show that ProtoPNet can achieve comparable accuracy with its analogous non-interpretable counterpart, and when several ProtoPNets are combined into a larger network, it can achieve an accuracy that is on par with some of the best-performing deep models. Moreover, ProtoPNet provides a level of interpretability that is absent in other interpretable deep models.},
  file = {/Users/khawla/Zotero/storage/EIPWC9TP/Chen et al. - 2019 - This Looks Like That Deep Learning for Interpreta.pdf}
}

@article{Choukali_2024_PseudoClassPartPrototype,
  title = {Pseudo-Class Part Prototype Networks for Interpretable Breast Cancer Classification},
  author = {Choukali, Mohammad Amin and Amirani, Mehdi Chehel and Valizadeh, Morteza and Abbasi, Ata and Komeili, Majid},
  year = {2024},
  month = may,
  journal = {Scientific Reports},
  volume = {14},
  number = {1},
  pages = {10341},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-024-60743-x},
  urldate = {2024-12-03},
  abstract = {Interpretability in machine learning has become increasingly important as machine learning is being used in more and more applications, including those with high-stakes consequences such as healthcare where Interpretability has been regarded as a key to the successful adoption of machine learning models. However, using confounding/irrelevant information in making predictions by deep learning models, even the interpretable ones, poses critical challenges to their clinical acceptance. That has recently drawn researchers' attention to issues beyond the mere interpretation of deep learning models. In this paper, we first investigate application of an inherently interpretable prototype-based architecture, known as ProtoPNet, for breast cancer classification in digital pathology and highlight its shortcomings in this application. Then, we propose a new method that uses more medically relevant information and makes more accurate and interpretable predictions. Our method leverages the clustering concept and implicitly increases the number of classes in the training dataset. The proposed method learns more relevant prototypes without any pixel-level annotated data. To have a more holistic assessment, in addition to classification accuracy, we define a new metric for assessing the degree of interpretability based on the comments of a group of skilled pathologists. Experimental results on the BreakHis dataset show that the proposed method effectively improves the classification accuracy and interpretability by respectively \$\$8 {\textbackslash}\%\$\$and \$\$18 {\textbackslash}\%\$\$. Therefore, the proposed method can be seen as a step toward implementing interpretable deep learning models for the detection of breast cancer using histopathology images.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Biomedical engineering,Breast cancer},
  file = {/Users/khawla/Zotero/storage/BBWRXJP3/Choukali et al. - 2024 - Pseudo-class part prototype networks for interpret.pdf}
}

@article{Davoodi_2023_InterpretabilityPartPrototypeBased,
  title = {On the Interpretability of Part-Prototype Based Classifiers: A Human Centric Analysis},
  shorttitle = {On the Interpretability of Part-Prototype Based Classifiers},
  author = {Davoodi, Omid and Mohammadizadehsamakosh, Shayan and Komeili, Majid},
  year = {2023},
  month = dec,
  journal = {Scientific Reports},
  volume = {13},
  number = {1},
  pages = {23088},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-023-49854-z},
  urldate = {2024-12-03},
  abstract = {Part-prototype networks have recently become methods of interest as an interpretable alternative to many of the current black-box image classifiers. However, the interpretability of these methods from the perspective of human users has not been sufficiently explored. In addition, previous works have had major issues with following proper experiment design and task representation that limit their reliability and validity. In this work, we have devised a framework for evaluating the interpretability of part-prototype-based models from a human perspective that solves these issues. The proposed framework consists of three actionable metrics and experiments. The results of these experiments will reveal important and reliable interpretability related properties of such models. To demonstrate the usefulness of our framework, we performed an extensive set of experiments using Amazon Mechanical Turk. They not only show the capability of our framework in assessing the interpretability of various part-prototype-based models, but they also are, to the best of our knowledge, the most comprehensive work on evaluating such methods in a unified framework.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Computer science,Information technology},
  file = {/Users/khawla/Zotero/storage/V8LAIZIM/Davoodi et al. - 2023 - On the interpretability of part-prototype based cl.pdf}
}

@inproceedings{Donnelly_2022_DeformableProtoPNetInterpretable,
  title = {Deformable {{ProtoPNet}}: {{An Interpretable Image Classifier Using Deformable Prototypes}}},
  shorttitle = {Deformable {{ProtoPNet}}},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Donnelly, Jon and Barnett, Alina Jade and Chen, Chaofan},
  year = {2022},
  month = jun,
  pages = {10255--10265},
  publisher = {IEEE},
  address = {New Orleans, LA, USA},
  doi = {10.1109/CVPR52688.2022.01002},
  urldate = {2024-12-03},
  abstract = {We present a deformable prototypical part network (Deformable ProtoPNet), an interpretable image classifier that integrates the power of deep learning and the interpretability of case-based reasoning. This model classifies input images by comparing them with prototypes learned during training, yielding explanations in the form of ``this looks like that.'' However, while previous methods use spatially rigid prototypes, we address this shortcoming by proposing spatially flexible prototypes. Each prototype is made up of several prototypical parts that adaptively change their relative spatial positions depending on the input image. Consequently, a Deformable ProtoPNet can explicitly capture pose variations and context, improving both model accuracy and the richness of explanations provided. Compared to other case-based interpretable models using prototypes, our approach achieves state-of-the-art accuracy and gives an explanation with greater context. The code is available at https://github.com/jdonnelly36/Deformable-ProtoPNet.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {978-1-66546-946-3},
  langid = {english},
  file = {/Users/khawla/Zotero/storage/CPD6VBGQ/Donnelly et al. - 2022 - Deformable ProtoPNet An Interpretable Image Class.pdf}
}

@inproceedings{Fauvel_2023_LightweightEfficientExplainablebyDesign,
  title = {A {{Lightweight}}, {{Efficient}} and {{Explainable-by-Design Convolutional Neural Network}} for {{Internet Traffic Classification}}},
  booktitle = {Proceedings of the 29th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Fauvel, Kevin and Chen, Fuxing and Rossi, Dario},
  year = {2023},
  month = aug,
  series = {{{KDD}} '23},
  pages = {4013--4023},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3580305.3599762},
  urldate = {2024-12-03},
  abstract = {Traffic classification, i.e., the identification of the type of applications flowing in a network, is a strategic task for numerous activities (e.g., intrusion detection, routing). This task faces some critical challenges that current deep learning approaches do not address. The design of current approaches do not take into consideration the fact that networking hardware (e.g., routers) often runs with limited computational resources. Further, they do not meet the need for faithful explainability highlighted by regulatory bodies. Finally, these traffic classifiers are evaluated on small datasets which fail to reflect the diversity of applications in real-world settings.Therefore, this paper introduces a new Lightweight, Efficient and eXplainable-by-design convolutional neural network (LEXNet) for Internet traffic classification, which relies on a new residual block (for lightweight and efficiency purposes) and prototype layer (for explainability). Based on a commercial-grade dataset, our evaluation shows that LEXNet succeeds to maintain the same accuracy as the best performing state-of-the-art neural network, while providing the additional features previously mentioned. Moreover, we illustrate the explainability feature of our approach, which stems from the communication of detected application prototypes to the end-user, and we highlight the faithfulness of LEXNet explanations through a comparison with post hoc methods.},
  isbn = {9798400701030},
  file = {/Users/khawla/Zotero/storage/R7NGF3QK/Fauvel et al. - 2023 - A Lightweight, Efficient and Explainable-by-Design.pdf}
}

@inproceedings{Gouvea_2023_InteractiveMachineLearning,
  title = {Interactive {{Machine Learning Solutions}} for {{Acoustic Monitoring}} of {{Animal Wildlife}} in {{Biosphere Reserves}}},
  booktitle = {Thirty-{{Second International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Gouv{\^e}a, Thiago S. and Kath, Hannes and Troshani, Ilira and L{\"u}ers, Bengt and Serafini, Patricia P. and Campos, Ivan B. and Afonso, Andr{\'e} S. and Leandro, Sergio M. F. M. and Swanepoel, Lourens and Theron, Nicholas and Swemmer, Anthony M. and Sonntag, Daniel},
  year = {2023},
  month = aug,
  volume = {6},
  pages = {6405--6413},
  issn = {1045-0823},
  doi = {10.24963/ijcai.2023/711},
  urldate = {2024-12-03},
  abstract = {Electronic proceedings of IJCAI 2023},
  langid = {english},
  file = {/Users/khawla/Zotero/storage/N8GT2KXZ/Gouvêa et al. - 2023 - Interactive Machine Learning Solutions for Acousti.pdf}
}

@article{Hong_2023_ProtoryNetInterpretableText,
  title = {{{ProtoryNet}} - {{Interpretable Text Classification Via Prototype Trajectories}}},
  author = {Hong, Dat},
  year = {2023},
  month = aug,
  journal = {Journal of Machine Learning Research},
  volume = {24},
  pages = {1--39},
  abstract = {We propose a novel interpretable deep neural network for text classification, called ProtoryNet, based on a new concept of prototype trajectories. Motivated by the prototype theory in modern linguistics, ProtoryNet makes a prediction by finding the most similar prototype for each sentence in a text sequence and feeding an RNN backbone with the proximity of each sentence to the corresponding active prototype. The RNN backbone then captures the temporal pattern of the prototypes, which we refer to as prototype trajectories. Prototype trajectories enable intuitive and fine-grained interpretation of the reasoning process of the RNN model, in resemblance to how humans analyze texts. We also design a prototype pruning procedure to reduce the total number of prototypes used by the model for better interpretability. Experiments on multiple public datasets demonstrate that ProtoryNet achieves higher accuracy than the baseline prototype-based deep neural net and narrows the performance gap when compared to state-of-the-art black-box models. In addition, after prototype pruning, the resulting ProtoryNet models only need less than or around 20 prototypes for all datasets, which significantly benefits interpretability. Furthermore, we report survey results indicating that human users find ProtoryNet more intuitive and easier to understand compared to other prototype-based methods.},
  langid = {english},
  file = {/Users/khawla/Zotero/storage/CIAF5NIG/Hong - ProtoryNet - Interpretable Text Classification Via.pdf}
}

@inproceedings{Huang_2023_EvaluationImprovementInterpretability,
  title = {Evaluation and {{Improvement}} of {{Interpretability}} for {{Self-Explainable Part-Prototype Networks}}},
  booktitle = {2023 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Huang, Qihan and Xue, Mengqi and Huang, Wenqi and Zhang, Haofei and Song, Jie and Jing, Yongcheng and Song, Mingli},
  year = {2023},
  month = oct,
  pages = {2011--2020},
  publisher = {IEEE},
  address = {Paris, France},
  doi = {10.1109/ICCV51070.2023.00192},
  urldate = {2024-12-03},
  abstract = {Part-prototype networks (e.g., ProtoPNet, ProtoTree, and ProtoPool) have attracted broad research interest for their intrinsic interpretability and comparable accuracy to noninterpretable counterparts. However, recent works find that the interpretability from prototypes is fragile, due to the semantic gap between the similarities in the feature space and that in the input space. In this work, we strive to address this challenge by making the first attempt to quantitatively and objectively evaluate the interpretability of the part-prototype networks. Specifically, we propose two evaluation metrics, termed as ``consistency score'' and ``stability score'', to evaluate the explanation consistency across images and the explanation robustness against perturbations, respectively, both of which are essential for explanations taken into practice. Furthermore, we propose an elaborated part-prototype network with a shallow-deep feature alignment (SDFA) module and a score aggregation (SA) module to improve the interpretability of prototypes. We conduct systematical evaluation experiments and provide substantial discussions to uncover the interpretability of existing part-prototype networks. Experiments on three benchmarks across nine architectures demonstrate that our model achieves significantly superior performance to the state of the art, in both the accuracy and interpretability. Our code is available at https://github.com/hqhQAQ/EvalProtoPNet.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {9798350307184},
  langid = {english},
  file = {/Users/khawla/Zotero/storage/H28B7VCE/Huang et al. - 2023 - Evaluation and Improvement of Interpretability for.pdf}
}

@inproceedings{Huang_2024_ConceptTrustworthinessConcept,
  title = {On the Concept Trustworthiness in Concept Bottleneck Models},
  booktitle = {Proceedings of the {{Thirty-Eighth AAAI Conference}} on {{Artificial Intelligence}} and {{Thirty-Sixth Conference}} on {{Innovative Applications}} of {{Artificial Intelligence}} and {{Fourteenth Symposium}} on {{Educational Advances}} in {{Artificial Intelligence}}},
  author = {Huang, Qihan and Song, Jie and Hu, Jingwen and Zhang, Haofei and Wang, Yong and Song, Mingli},
  year = {2024},
  month = feb,
  series = {{{AAAI}}'24/{{IAAI}}'24/{{EAAI}}'24},
  volume = {38},
  pages = {21161--21168},
  publisher = {AAAI Press},
  doi = {10.1609/aaai.v38i19.30109},
  urldate = {2025-01-28},
  abstract = {Concept Bottleneck Models (CBMs), which break down the reasoning process into the input-to-concept mapping and the concept-to-label prediction, have garnered significant attention due to their remarkable interpretability achieved by the interpretable concept bottleneck. However, despite the transparency of the concept-to-label prediction, the mapping from the input to the intermediate concept remains a black box, giving rise to concerns about the trustworthiness of the learned concepts (i.e., these concepts may be predicted based on spurious cues). The issue of concept untrustworthiness greatly hampers the interpretability of CBMs, thereby hindering their further advancement. To conduct a comprehensive analysis on this issue, in this study we establish a benchmark to assess the trustworthiness of concepts in CBMs. A pioneering metric, referred to as concept trustworthiness score, is proposed to gauge whether the concepts are derived from relevant regions. Additionally, an enhanced CBM is introduced, enabling concept predictions to be made specifically from distinct parts of the feature map, thereby facilitating the exploration of their related regions. Besides, we introduce three modules, namely the cross-layer alignment (CLA) module, the cross-image alignment (CIA) module, and the prediction alignment (PA) module, to further enhance the concept trustworthiness within the elaborated CBM. The experiments on five datasets across ten architectures demonstrate that without using any concept localization annotations during training, our model improves the concept trustworthiness by a large margin, meanwhile achieving superior accuracy to the state-of-the-arts. Our code is available at https://github.com/hqhQAQ/ProtoCBM.},
  isbn = {978-1-57735-887-9},
  file = {/Users/khawla/Zotero/storage/SSHVPBIV/Huang et al. - 2024 - On the concept trustworthiness in concept bottlene.pdf}
}

@inproceedings{Keswani_2022_Proto2ProtoCanyou,
  title = {{{Proto2Proto}}: {{Can}} You Recognize the Car, the Way {{I}} Do?},
  shorttitle = {{{Proto2Proto}}},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Keswani, Monish and Ramakrishnan, Sriranjani and Reddy, Nishant and Balasubramanian, Vineeth N},
  year = {2022},
  month = jun,
  pages = {10223--10233},
  publisher = {IEEE},
  address = {New Orleans, LA, USA},
  doi = {10.1109/CVPR52688.2022.00999},
  urldate = {2024-12-03},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {978-1-66546-946-3},
  langid = {english},
  file = {/Users/khawla/Zotero/storage/UZS3JEXM/Keswani et al. - 2022 - Proto2Proto Can you recognize the car, the way I .pdf}
}

@inproceedings{Kim_2021_XProtoNetDiagnosisChest,
  title = {{{XProtoNet}}: {{Diagnosis}} in {{Chest Radiography}} with {{Global}} and {{Local Explanations}}},
  shorttitle = {{{XProtoNet}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Kim, Eunji and Kim, Siwon and Seo, Minji and Yoon, Sungroh},
  year = {2021},
  month = jun,
  pages = {15714--15723},
  publisher = {IEEE},
  address = {Nashville, TN, USA},
  doi = {10.1109/CVPR46437.2021.01546},
  urldate = {2024-12-03},
  abstract = {Automated diagnosis using deep neural networks in chest radiography can help radiologists detect lifethreatening diseases. However, existing methods only provide predictions without accurate explanations, undermining the trustworthiness of the diagnostic methods. Here, we present XProtoNet, a globally and locally interpretable diagnosis framework for chest radiography. XProtoNet learns representative patterns of each disease from X-ray images, which are prototypes, and makes a diagnosis on a given Xray image based on the patterns. It predicts the area where a sign of the disease is likely to appear and compares the features in the predicted area with the prototypes. It can provide a global explanation, the prototype, and a local explanation, how the prototype contributes to the prediction of a single image. Despite the constraint for interpretability, XProtoNet achieves state-of-the-art classification performance on the public NIH chest X-ray dataset.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn = {978-1-66544-509-2},
  langid = {english},
  file = {/Users/khawla/Zotero/storage/X4LZGI6J/Kim et al. - 2021 - XProtoNet Diagnosis in Chest Radiography with Glo.pdf}
}

@inproceedings{Kim_2022_HIVEEvaluatingHuman,
  title = {{{HIVE}}: {{Evaluating}} the {{Human Interpretability}} of {{Visual Explanations}}},
  shorttitle = {{{HIVE}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2022},
  author = {Kim, Sunnie S. Y. and Meister, Nicole and Ramaswamy, Vikram V. and Fong, Ruth and Russakovsky, Olga},
  editor = {Avidan, Shai and Brostow, Gabriel and Ciss{\'e}, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
  year = {2022},
  pages = {280--298},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-19775-8_17},
  abstract = {As AI technology is increasingly applied to high-impact, high-risk domains, there have been a number of new methods aimed at making AI models more human interpretable. Despite the recent growth of interpretability work, there is a lack of systematic evaluation of proposed techniques. In this work, we introduce HIVE (Human Interpretability of Visual Explanations), a novel human evaluation framework that assesses the utility of explanations to human users in AI-assisted decision making scenarios, and enables falsifiable hypothesis testing, cross-method comparison, and human-centered evaluation of visual interpretability methods. To the best of our knowledge, this is the first work of its kind. Using HIVE, we conduct IRB-approved human studies with nearly 1000 participants and evaluate four methods that represent the diversity of computer vision interpretability works: GradCAM, BagNet, ProtoPNet, and ProtoTree. Our results suggest that explanations engender human trust, even for incorrect predictions, yet are not distinct enough for users to distinguish between correct and incorrect predictions. We open-source HIVE to enable future studies and encourage more human-centered approaches to interpretability research. HIVE can be found at https://princetonvisualai.github.io/HIVE.},
  isbn = {978-3-031-19775-8},
  langid = {english},
  keywords = {Evaluation framework,Explainable AI (XAI),Human studies,Human-centered AI,Interpretability}
}

@inproceedings{Kim_2022_ViTNeTInterpretableVision,
  title = {{{ViT-NeT}}: {{Interpretable Vision Transformers}} with {{Neural Tree Decoder}}},
  shorttitle = {{{ViT-NeT}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Kim, Sangwon and Nam, Jaeyeal and Ko, Byoung Chul},
  year = {2022},
  month = jun,
  pages = {11162--11172},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-12-03},
  abstract = {Vision transformers (ViTs), which have demonstrated a state-of-the-art performance in image classification, can also visualize global interpretations through attention-based contributions. However, the complexity of the model makes it difficult to interpret the decision-making process, and the ambiguity of the attention maps can cause incorrect correlations between image patches. In this study, we propose a new ViT neural tree decoder (ViT-NeT). A ViT acts as a backbone, and to solve its limitations, the output contextual image patches are applied to the proposed NeT. The NeT aims to accurately classify fine-grained objects with similar inter-class correlations and different intra-class correlations. In addition, it describes the decision-making process through a tree structure and prototype and enables a visual interpretation of the results. The proposed ViT-NeT is designed to not only improve the classification performance but also provide a human-friendly interpretation, which is effective in resolving the trade-off between performance and interpretability. We compared the performance of ViT-NeT with other state-of-art methods using widely used fine-grained visual categorization benchmark datasets and experimentally proved that the proposed method is superior in terms of the classification performance and interpretability. The code and models are publicly available at https://github.com/jumpsnack/ViT-NeT.},
  langid = {english},
  file = {/Users/khawla/Zotero/storage/L5MCV9JJ/Kim et al. - 2022 - ViT-NeT Interpretable Vision Transformers with Ne.pdf}
}

@article{Li_2021_ImprovingPrototypicalVisual,
  author = {Li, Aaron J. and Netzorg, Robin and Cheng, Zhihan and Zhang, Zhuoqin and Yu, Bin}, 
  title = {Improving prototypical visual explanations with reward reweighing, reselection, and retraining}, 
  year = {2024}, 
  publisher = {JMLR.org}, 
  abstract = {In recent years, work has gone into developing deep interpretable methods for image classification that clearly attributes a model's output to specific features of the data. One such of these methods is the prototypical part network (ProtoPNet), which attempts to classify images based on meaningful parts of the input. While this architecture is able to produce visually interpretable classifications, it often learns to classify based on parts of the image that are not semantically meaningful. To address this problem, we propose the reward reweighing, reselecting, and retraining (R3) post-processing framework, which performs three additional corrective updates to a pretrained ProtoPNet in an offline and efficient manner. The first two steps involve learning a reward model based on collected human feedback and then aligning the prototypes with human preferences. The final step is retraining, which realigns the base features and the classifier layer of the original model with the updated prototypes. We find that our R3 framework consistently improves both the interpretability and the predictive accuracy of ProtoPNet and its variants.}, booktitle = {Proceedings of the 41st International Conference on Machine Learning}, 
  articleno = {1143}, 
  numpages = {14}, 
  location = {Vienna, Austria}, 
  series = {ICML'24}
}

@incollection{Li_2022_WeaklySupervisedTemporalAction,
  title = {Weakly-{{Supervised Temporal Action Detection}} for {{Fine-Grained Videos}} with {{Hierarchical Atomic Actions}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2022},
  author = {Li, Zhi and He, Lu and Xu, Huijuan},
  editor = {Avidan, Shai and Brostow, Gabriel and Ciss{\'e}, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
  year = {2022},
  volume = {13670},
  pages = {567--584},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-20080-9_33},
  urldate = {2024-12-03},
  abstract = {Action understanding has evolved into the era of fine granularity, as most human behaviors in real life have only minor differences. To detect these fine-grained actions accurately in a label-efficient way, we tackle the problem of weakly-supervised fine-grained temporal action detection in videos for the first time. Without the careful design to capture subtle differences between fine-grained actions, previous weaklysupervised models for general action detection cannot perform well in the fine-grained setting. We propose to model actions as the combinations of reusable atomic actions which are automatically discovered from data through self-supervised clustering, in order to capture the commonality and individuality of fine-grained actions. The learnt atomic actions, represented by visual concepts, are further mapped to fine and coarse action labels leveraging the semantic label hierarchy. Our approach constructs a visual representation hierarchy of four levels: clip level, atomic action level, fine action class level and coarse action class level, with supervision at each level. Extensive experiments on two large-scale fine-grained video datasets, FineAction and FineGym, show the benefit of our proposed weakly-supervised model for fine-grained action detection, and it achieves state-of-the-art results.},
  isbn = {978-3-031-20079-3 978-3-031-20080-9},
  langid = {english},
  file = {/Users/khawla/Zotero/storage/S2SL336J/Li et al. - 2022 - Weakly-Supervised Temporal Action Detection for Fi.pdf}
}

@inproceedings{Ma_2023_ThisLooksThose,
  title = {This {{Looks Like Those}}: {{Illuminating Prototypical Concepts Using Multiple Visualizations}}},
  shorttitle = {This {{Looks Like Those}}},
  booktitle = {Thirty-Seventh {{Conference}} on {{Neural Information Processing Systems}}},
  author = {Ma, Chiyu and Zhao, Brandon and Chen, Chaofan and Rudin, Cynthia},
  year = {2023},
  month = nov,
  urldate = {2024-12-03},
  abstract = {We present ProtoConcepts, a method for interpretable image classification combining deep learning and case-based reasoning using prototypical parts. Existing work in prototype-based image classification uses a "this looks like that'' reasoning process, which dissects a test image by finding prototypical parts and combining evidence from these prototypes to make a final classification. However, all of the existing prototypical part-based image classifiers provide only one-to-one comparisons, where a single training image patch serves as a prototype to compare with a part of our test image. With these single-image comparisons, it can often be difficult to identify the underlying concept being compared (e.g., "is it comparing the color or the shape?''). Our proposed method modifies the architecture of prototype-based networks to instead learn prototypical concepts which are visualized using multiple image patches. Having multiple visualizations of the same prototype allows us to more easily identify the concept captured by that prototype (e.g., "the test image and the related training patches are all the same shade of blue''), and allows our model to create richer, more interpretable visual explanations. Our experiments show that our ``this looks like those'' reasoning process can be applied as a modification to a wide range of existing prototypical image classification networks while achieving comparable accuracy on benchmark datasets.},
  langid = {english},
  file = {/Users/khawla/Zotero/storage/R9XIWSCS/Ma et al. - 2023 - This Looks Like Those Illuminating Prototypical C.pdf}
}

@inproceedings{Ma_2024_InterpretableImageClassification,
  title = {Interpretable {{Image Classification}} with {{Adaptive Prototype-based Vision Transformers}}},
  booktitle = {The {{Thirty-eighth Annual Conference}} on {{Neural Information Processing Systems}}},
  author = {Ma, Chiyu and Donnelly, Jon and Liu, Wenjun and Vosoughi, Soroush and Rudin, Cynthia and Chen, Chaofan},
  year = {2024},
  month = nov,
  urldate = {2025-01-06},
  abstract = {We present ProtoViT, a method for interpretable image classification combining deep learning and case-based reasoning. This method classifies an image by comparing it to a set of learned prototypes, providing explanations of the form ``this looks like that.'' In our model, a prototype consists of **parts**, which can deform over irregular geometries to create a better comparison between images. Unlike existing models that rely on Convolutional Neural Network (CNN) backbones and spatially rigid prototypes, our model integrates Vision Transformer (ViT) backbones into prototype based models, while offering spatially deformed prototypes that not only accommodate geometric variations of objects but also provide coherent and clear prototypical feature representations with an adaptive number of prototypical parts. Our experiments show that our model can generally achieve higher performance than the existing prototype based models. Our comprehensive analyses ensure that the prototypes are consistent and the interpretations are faithful.},
  langid = {english},
  file = {/Users/khawla/Zotero/storage/DHPDUXMR/Ma et al. - 2024 - Interpretable Image Classification with Adaptive P.pdf}
}

@inproceedings{Nauta_2021_NeuralPrototypeTrees,
  title = {Neural {{Prototype Trees}} for {{Interpretable Fine-grained Image Recognition}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Nauta, Meike and Van Bree, Ron and Seifert, Christin},
  year = {2021},
  month = jun,
  pages = {14928--14938},
  publisher = {IEEE},
  address = {Nashville, TN, USA},
  doi = {10.1109/CVPR46437.2021.01469},
  urldate = {2024-12-03},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn = {978-1-66544-509-2},
  langid = {english},
  file = {/Users/khawla/Zotero/storage/92SBMTMA/Nauta et al. - 2021 - Neural Prototype Trees for Interpretable Fine-grai.pdf}
}

@misc{Nauta_2023_Co12RecipeEvaluating,
  title = {The {{Co-12 Recipe}} for {{Evaluating Interpretable Part-Prototype Image Classifiers}}},
  author = {Nauta, Meike and Seifert, Christin},
  booktitle={Explainable Artificial Intelligence},
  publisher={Springer Nature Switzerland},
  address={Cham},
  pages={397--420},
  year = {2023},
  month = jul,
  abstract = {Interpretable part-prototype models are computer vision models that are explainable by design. The models learn prototypical parts and recognise these components in an image, thereby combining classification and explanation. Despite the recent attention for intrinsically interpretable models, there is no comprehensive overview on evaluating the explanation quality of interpretable part-prototype models. Based on the Co-12 properties for explanation quality as introduced in arXiv:2201.08164 (e.g., correctness, completeness, compactness), we review existing work that evaluates part-prototype models, reveal research gaps and outline future approaches for evaluation of the explanation quality of part-prototype models. This paper, therefore, contributes to the progression and maturity of this relatively new research field on interpretable part-prototype models. We additionally provide a ``Co-12 cheat sheet'' that acts as a concise summary of our findings on evaluating part-prototype models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/khawla/Zotero/storage/EFIT5VIT/Nauta and Seifert - 2023 - The Co-12 Recipe for Evaluating Interpretable Part.pdf;/Users/khawla/Zotero/storage/HM8MNPAC/2307.html}
}

@inproceedings{Nauta_2023_PIPNetPatchBasedIntuitive,
  title = {{{PIP-Net}}: {{Patch-Based Intuitive Prototypes}} for {{Interpretable Image Classification}}},
  shorttitle = {{{PIP-Net}}},
  booktitle = {2023 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Nauta, Meike and Schl{\"o}tterer, J{\"o}rg and Van Keulen, Maurice and Seifert, Christin},
  year = {2023},
  pages = {2744--2753},
  publisher = {IEEE},
  address = {Vancouver, BC, Canada},
  doi = {10.1109/CVPR52729.2023.00269},
  urldate = {2024-12-03},
  abstract = {Interpretable methods based on prototypical patches recognize various components in an image in order to explain their reasoning to humans. However, existing prototypebased methods can learn prototypes that are not in line with human visual perception, i.e., the same prototype can refer to different concepts in the real world, making interpretation not intuitive. Driven by the principle of explainability-bydesign, we introduce PIP-Net (Patch-based Intuitive Prototypes Network): an interpretable image classification model that learns prototypical parts in a self-supervised fashion which correlate better with human vision. PIP-Net can be interpreted as a sparse scoring sheet where the presence of a prototypical part in an image adds evidence for a class. The model can also abstain from a decision for out-ofdistribution data by saying ``I haven't seen this before''. We only use image-level labels and do not rely on any part annotations. PIP-Net is globally interpretable since the set of learned prototypes shows the entire reasoning of the model. A smaller local explanation locates the relevant prototypes in one image. We show that our prototypes correlate with ground-truth object parts, indicating that PIP-Net closes the ``semantic gap'' between latent space and pixel space. Hence, our PIP-Net with interpretable prototypes enables users to interpret the decision making process in an intuitive, faithful and semantically meaningful way. Code is available at https://github.com/M-Nauta/PIPNet.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {9798350301298},
  langid = {english},
  file = {/Users/khawla/Zotero/storage/HR2WP2Y9/Nauta et al. - 2023 - PIP-Net Patch-Based Intuitive Prototypes for Inte.pdf}
}

@misc{Pathak_2024_PrototypeBasedInterpretableBreast,
  title = {Prototype-Based {{Interpretable Breast Cancer Prediction Models}}: {{Analysis}} and {{Challenges}}},
  booktitle = {Explainable Artificial Intelligence},
  author = {Pathak, Shreyasi and Schl{\"o}tterer, J{\"o}rg and Veltman, Jeroen and Geerdink, Jeroen and van Keulen, Maurice and Seifert, Christin},
  year = {2024},
  month = jul,
  pages = {41--42},
  publisher = {Springer Nature Switzerland},
  abstract = {Deep learning models have achieved high performance in medical applications, however, their adoption in clinical practice is hindered due to their black-box nature. Self-explainable models, like prototype-based models, can be especially beneficial as they are interpretable by design. However, if the learnt prototypes are of low quality then the prototype-based models are as good as black-box. Having high quality prototypes is a pre-requisite for a truly interpretable model. In this work, we propose a prototype evaluation framework for coherence (PEF-C) for quantitatively evaluating the quality of the prototypes based on domain knowledge. We show the use of PEF-C in the context of breast cancer prediction using mammography. Existing works on prototype-based models on breast cancer prediction using mammography have focused on improving the classification performance of prototype-based models compared to black-box models and have evaluated prototype quality through anecdotal evidence. We are the first to go beyond anecdotal evidence and evaluate the quality of the mammography prototypes systematically using our PEF-C. Specifically, we apply three state-of-the-art prototype-based models, ProtoPNet, BRAIxProtoPNet++ and PIP-Net on mammography images for breast cancer prediction and evaluate these models w.r.t. i) classification performance, and ii) quality of the prototypes, on three public datasets. Our results show that prototype-based models are competitive with black-box models in terms of classification performance, and achieve a higher score in detecting ROIs. However, the quality of the prototypes are not yet sufficient and can be improved in aspects of relevance, purity and learning a variety of prototypes. We call the XAI community to systematically evaluate the quality of the prototypes to check their true usability in high stake decisions and improve such models further.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{Ruis_2021_IndependentPrototypePropagation,
  title = {Independent {{Prototype Propagation}} for {{Zero-Shot Compositionality}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ruis, Frank and Burghouts, Gertjan and Bucur, Doina},
  year = {2021},
  volume = {34},
  pages = {10641--10653},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-12-03},
  abstract = {Humans are good at compositional zero-shot reasoning; someone who has never seen a zebra before could nevertheless recognize one when we tell them it looks like a horse with black and white stripes. Machine learning systems, on the other hand, usually leverage spurious correlations in the training data, and while such correlations can help recognize objects in context, they hurt generalization. To be able to deal with underspecified datasets while still leveraging contextual clues during classification, we propose ProtoProp, a novel prototype propagation graph method. First we learn prototypical representations of objects (e.g., zebra) that are independent w.r.t. their attribute labels (e.g., stripes) and vice versa. Next we propagate the independent prototypes through a compositional graph, to learn compositional prototypes of novel attribute-object combinations that reflect the dependencies of the target distribution. The method does not rely on any external data, such as class hierarchy graphs or pretrained word embeddings. We evaluate our approach on AO-Clevr, a synthetic and strongly visual dataset with clean labels, UT-Zappos, a noisy real-world dataset of fine-grained shoe types, and C-GQA, a large-scale object detection dataset modified for compositional zero-shot learning. We show that in the generalized compositional zero-shot setting we outperform state-of-the-art results, and through ablations we show the importance of each part of the method and their contribution to the final results. The code is available on github.},
  file = {/Users/khawla/Zotero/storage/X8W8VAKW/Ruis et al. - 2021 - Independent Prototype Propagation for Zero-Shot Co.pdf}
}

@inproceedings{Rymarczyk_2021_ProtoPSharePrototypicalParts,
  title = {{{ProtoPShare}}: {{Prototypical Parts Sharing}} for {{Similarity Discovery}} in {{Interpretable Image Classification}}},
  shorttitle = {{{ProtoPShare}}},
  booktitle = {Proceedings of the 27th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Rymarczyk, Dawid and Struski, {\L}ukasz and Tabor, Jacek and Zieli{\'n}ski, Bartosz},
  year = {2021},
  month = aug,
  series = {{{KDD}} '21},
  pages = {1420--1430},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3447548.3467245},
  urldate = {2024-12-03},
  abstract = {In this work, we introduce an extension to ProtoPNet called ProtoPShare which shares prototypical parts between classes. To obtain prototype sharing we prune prototypical parts using a novel data-dependent similarity. Our approach substantially reduces the number of prototypes needed to preserve baseline accuracy and finds prototypical similarities between classes. We show the effectiveness of ProtoPShare on the CUB-200-2011 and the Stanford Cars datasets and confirm the semantic consistency of its prototypical parts in user-study.},
  isbn = {978-1-4503-8332-5},
  file = {/Users/khawla/Zotero/storage/Y47YLL3E/Rymarczyk et al. - 2021 - ProtoPShare Prototypical Parts Sharing for Simila.pdf}
}

@incollection{Rymarczyk_2022_InterpretableImageClassification,
  title = {Interpretable {{Image Classification}} with {{Differentiable Prototypes Assignment}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2022},
  author = {Rymarczyk, Dawid and Struski, {\L}ukasz and G{\'o}rszczak, Micha{\l} and Lewandowska, Koryna and Tabor, Jacek and Zieli{\'n}ski, Bartosz},
  editor = {Avidan, Shai and Brostow, Gabriel and Ciss{\'e}, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
  year = {2022},
  volume = {13672},
  pages = {351--368},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-19775-8_21},
  urldate = {2024-12-03},
  abstract = {Existing prototypical-based models address the black-box nature of deep learning. However, they are sub-optimal as they often assume separate prototypes for each class, require multi-step optimization, make decisions based on prototype absence (so-called negative reasoning process), and derive vague prototypes. To address those shortcomings, we introduce ProtoPool, an interpretable prototype-based model with positive reasoning and three main novelties. Firstly, we reuse prototypes in classes, which significantly decreases their number. Secondly, we allow automatic, fully differentiable assignment of prototypes to classes, which substantially simplifies the training process. Finally, we propose a new focal similarity function that contrasts the prototype from the background and consequently concentrates on more salient visual features. We show that ProtoPool obtains state-of-the-art accuracy on the CUB-200-2011 and the Stanford Cars datasets, substantially reducing the number of prototypes. We provide a theoretical analysis of the method and a user study to show that our prototypes capture more salient features than those obtained with competitive methods. We made the code available at https://github.com/gmum/ProtoPool.},
  isbn = {978-3-031-19774-1 978-3-031-19775-8},
  langid = {english},
  file = {/Users/khawla/Zotero/storage/Y5B9TVFM/Rymarczyk et al. - 2022 - Interpretable Image Classification with Differenti.pdf}
}

@inproceedings{Rymarczyk_2023_ICICLEInterpretableClass,
  title = {{{ICICLE}}: {{Interpretable Class Incremental Continual Learning}}},
  shorttitle = {{{ICICLE}}},
  booktitle = {2023 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Rymarczyk, Dawid and Van De Weijer, Joost and Zieli{\'n}ski, Bartosz and Twardowski, Bart{\l}omiej},
  year = {2023},
  month = oct,
  pages = {1887--1898},
  publisher = {IEEE},
  address = {Paris, France},
  doi = {10.1109/ICCV51070.2023.00181},
  urldate = {2024-12-03},
  abstract = {Continual learning enables incremental learning of new tasks without forgetting those previously learned, resulting in positive knowledge transfer that can enhance performance on both new and old tasks. However, continual learning poses new challenges for interpretability, as the rationale behind model predictions may change over time, leading to interpretability concept drift. We address this problem by proposing Interpretable Class-InCremental LEarning (ICICLE), an exemplar-free approach that adopts a prototypical part-based approach. It consists of three crucial novelties: interpretability regularization that distills previously learned concepts while preserving user-friendly positive reasoning; proximity-based prototype initialization strategy dedicated to the fine-grained setting; and taskrecency bias compensation devoted to prototypical parts. Our experimental results demonstrate that ICICLE reduces the interpretability concept drift and outperforms the existing exemplar-free methods of common class-incremental learning when applied to concept-based models.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {9798350307184},
  langid = {english},
  file = {/Users/khawla/Zotero/storage/XKSDLPLY/Rymarczyk et al. - 2023 - ICICLE Interpretable Class Incremental Continual .pdf}
}

@inproceedings{Rymarczyk_2023_ProtoMILMultipleInstance,
  title = {{{ProtoMIL}}: {{Multiple Instance Learning}} with~{{Prototypical Parts}} for~{{Whole-Slide Image Classification}}},
  shorttitle = {{{ProtoMIL}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Rymarczyk, Dawid and Pardyl, Adam and Kraus, Jaros{\l}aw and Kaczy{\'n}ska, Aneta and Skomorowski, Marek and Zieli{\'n}ski, Bartosz},
  editor = {Amini, Massih-Reza and Canu, St{\'e}phane and Fischer, Asja and Guns, Tias and Kralj Novak, Petra and Tsoumakas, Grigorios},
  year = {2023},
  pages = {421--436},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-031-26387-3_26},
  abstract = {The rapid development of histopathology scanners allowed the digital transformation of pathology. Current devices fastly and accurately digitize histology slides on many magnifications, resulting in whole slide images (WSI). However, direct application of supervised deep learning methods to WSI highest magnification is impossible due to hardware limitations. That is why WSI classification is usually analyzed using standard Multiple Instance Learning (MIL) approaches, that do not explain their predictions, which is crucial for medical applications. In this work, we fill this gap by introducing ProtoMIL, a novel self-explainable MIL method inspired by the case-based reasoning process that operates on visual prototypes. Thanks to incorporating prototypical features into objects description, ProtoMIL unprecedentedly joins the model accuracy and fine-grained interpretability, as confirmed by the experiments conducted on five recognized whole-slide image datasets.},
  isbn = {978-3-031-26387-3},
  langid = {english},
  keywords = {Digital pathology,Interpretable deep learning,Multiple instance learning},
  file = {/Users/khawla/Zotero/storage/HR5UFIFG/Rymarczyk et al. - 2023 - ProtoMIL Multiple Instance Learning with Prototyp.pdf}
}

@book{Sacha_2024_InterpretabilityBenchmarkEvaluating,
  title = {Interpretability Benchmark for Evaluating Spatial Misalignment of Prototypical Parts Explanations},
  author = {Sacha, Miko{\l}aj and Jura, Bartosz and Rymarczyk, Dawid and Struski, {\L}ukasz and Tabor, Jacek and Zieli{\'n}ski, Bartosz},
  year = {2024},
  publisher = {AAAI Press},
  doi = {10.1609/aaai.v38i19.30154},
  urldate = {2024-12-03},
  abstract = {{$<$}jats:p{$>$}Prototypical parts-based networks are becoming increasingly popular due to their faithful self-explanations. However, their similarity maps are calculated in the penultimate network layer. Therefore, the receptive field of the prototype activation region often depends on parts of the image outside this region, which can lead to misleading interpretations. We name this undesired behavior a spatial explanation misalignment and introduce an interpretability benchmark with a set of dedicated metrics for quantifying this phenomenon. In addition, we propose a method for misalignment compensation and apply it to existing state-of-the-art models. We show the expressiveness of our benchmark and the effectiveness of the proposed compensation methodology through extensive empirical studies.{$<$}/jats:p{$>$}},
  isbn = {978-1-57735-887-9},
  langid = {english},
  file = {/Users/khawla/Zotero/storage/KSRC6IM7/Sacha et al. - 2024 - Interpretability benchmark for evaluating spatial .pdf;/Users/khawla/Zotero/storage/3M4ZMBAG/e06bd5d4-f1ca-41af-99f9-fe494a07165b.html}
}

@inproceedings{Song_2024_MorphologicalPrototypingUnsupervised,
  title = {Morphological {{Prototyping}} for {{Unsupervised Slide Representation Learning}} in {{Computational Pathology}}},
  booktitle = {2024 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Song, Andrew H. and Chen, Richard J. and Ding, Tong and Williamson, Drew F.K. and Jaume, Guillaume and Mahmood, Faisal},
  year = {2024},
  month = jun,
  pages = {11566--11578},
  publisher = {IEEE},
  address = {Seattle, WA, USA},
  doi = {10.1109/CVPR52733.2024.01099},
  urldate = {2024-12-03},
  abstract = {Representation learning of pathology whole-slide images (WSIs) has been has primarily relied on weak supervision with Multiple Instance Learning (MIL). However, the slide representations resulting from this approach are highly tailored to specific clinical tasks, which limits their expressivity and generalization, particularly in scenarios with limited data. Instead, we hypothesize that morphological redundancy in tissue can be leveraged to build a task-agnostic slide representation in an unsupervised fashion. To this end, we introduce PANTHER, a prototypebased approach rooted in the Gaussian mixture model that summarizes the set of WSI patches into a much smaller set of morphological prototypes. Specifically, each patch is assumed to have been generated from a mixture distribution, where each mixture component represents a morphological exemplar. Utilizing the estimated mixture parameters, we then construct a compact slide representation that can be readily used for a wide range of downstream tasks. By performing an extensive evaluation of PANTHER on subtyping and survival tasks using 13 datasets, we show that 1) PANTHER outperforms or is on par with supervised MIL baselines and 2) the analysis of morphological prototypes brings new qualitative and quantitative insights into model interpretability. The code is available at https://github.com/mahmoodlab/Panther.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {9798350353006},
  langid = {english},
  file = {/Users/khawla/Zotero/storage/DCKF8HKR/Song et al. - 2024 - Morphological Prototyping for Unsupervised Slide R.pdf}
}

@inproceedings{Wang_2021_InterpretableImageRecognition,
  title = {Interpretable {{Image Recognition}} by {{Constructing Transparent Embedding Space}}},
  booktitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Wang, Jiaqi and Liu, Huafeng and Wang, Xinyue and Jing, Liping},
  year = {2021},
  pages = {875--884},
  publisher = {IEEE},
  address = {Montreal, QC, Canada},
  doi = {10.1109/ICCV48922.2021.00093},
  urldate = {2024-12-03},
  abstract = {Humans usually explain their reasoning (e.g. classification) by dissecting the image and pointing out the evidence from these parts to the concepts in their minds. Inspired by this cognitive process, several part-level interpretable neural network architectures have been proposed to explain the predictions. However, they suffer from the complex data structure and confusing the effect of the individual part to output category. In this work, an interpretable image recognition deep network is designed by introducing a plug-in transparent embedding space (TesNet) to bridge the highlevel input patches (e.g. CNN feature maps) and the output categories. This plug-in embedding space is spanned by transparent basis concepts which are constructed on the Grassmann manifold. These basis concepts are enforced to be category-aware and within-category concepts are orthogonal to each other, which makes sure the embedding space is disentangled. Meanwhile, each basis concept can be traced back to the particular image patches, thus they are transparent and friendly to explain the reasoning process. By comparing with state-of-the-art interpretable methods, TesNet is much more beneficial to classification tasks, esp. providing better interpretability on predictions and improve the final accuracy. The code is available at https://github.com/JackeyWang96/TesNet.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {978-1-66542-812-5},
  langid = {english},
  file = {/Users/khawla/Zotero/storage/9U5XYMLD/Wang et al. - 2021 - Interpretable Image Recognition by Constructing Tr.pdf}
}

@inproceedings{Wang_2023_LearningSupportTrivial,
  title = {Learning {{Support}} and {{Trivial Prototypes}} for {{Interpretable Image Classification}}},
  booktitle = {2023 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Wang, Chong and Liu, Yuyuan and Chen, Yuanhong and Liu, Fengbei and Tian, Yu and McCarthy, Davis and Frazer, Helen and Carneiro, Gustavo},
  year = {2023},
  month = oct,
  pages = {2062--2072},
  publisher = {IEEE},
  address = {Paris, France},
  doi = {10.1109/ICCV51070.2023.00197},
  urldate = {2024-12-03},
  abstract = {Prototypical part network (ProtoPNet) methods have been designed to achieve interpretable classification by associating predictions with a set of training prototypes, which we refer to as trivial prototypes because they are trained to lie far from the classification boundary in the feature space. Note that it is possible to make an analogy between ProtoPNet and support vector machine (SVM) given that the classification from both methods relies on computing similarity with a set of training points (i.e., trivial prototypes in ProtoPNet, and support vectors in SVM). However, while trivial prototypes are located far from the classification boundary, support vectors are located close to this boundary, and we argue that this discrepancy with the well-established SVM theory can result in ProtoPNet models with inferior classification accuracy. In this paper, we aim to improve the classification of ProtoPNet with a new method to learn support prototypes that lie near the classification boundary in the feature space, as suggested by the SVM theory. In addition, we target the improvement of classification results with a new model, named ST-ProtoPNet, which exploits our support prototypes and the trivial prototypes to provide more effective classification. Experimental results on CUB-200-2011, Stanford Cars, and Stanford Dogs datasets demonstrate that ST-ProtoPNet achieves state-of-the-art classification accuracy and interpretability results. We also show that the proposed support prototypes tend to be better localised in the object of interest rather than in the background region.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {9798350307184},
  langid = {english},
  file = {/Users/khawla/Zotero/storage/H6UXJWQL/Wang et al. - 2023 - Learning Support and Trivial Prototypes for Interp.pdf}
}

@inproceedings{Wang_2023_PROMINETPrototypebasedMultiView,
  title = {{{PROMINET}}: {{Prototype-based Multi-View Network}} for {{Interpretable Email Response Prediction}}},
  shorttitle = {{{PROMINET}}},
  booktitle = {Proceedings of the 2023 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}: {{Industry Track}}},
  author = {Wang, Yuqing and Vijayaraghavan, Prashanth and Degan, Ehsan},
  editor = {Wang, Mingxuan and Zitouni, Imed},
  year = {2023},
  month = dec,
  pages = {202--215},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-industry.21},
  urldate = {2024-12-03},
  abstract = {Email is a widely used tool for business communication, and email marketing has emerged as a cost-effective strategy for enterprises. While previous studies have examined factors affecting email marketing performance, limited research has focused on understanding email response behavior by considering email content and metadata. This study proposes a Prototype-based Multi-view Network (PROMINET) that incorporates semantic and structural information from email data. By utilizing prototype learning, the PROMINET model generates latent exemplars, enabling interpretable email response prediction. The model maps learned semantic and structural exemplars to observed samples in the training data at different levels of granularity, such as document, sentence, or phrase. The approach is evaluated on two real-world email datasets: the Enron corpus and an in-house Email Marketing corpus. Experimental results demonstrate that the PROMINET model outperforms baseline models, achieving a {\textbackslash}textasciitilde3\% improvement in F1 score on both datasets. Additionally, the model provides interpretability through prototypes at different granularity levels while maintaining comparable performance to non-interpretable models. The learned prototypes also show potential for generating suggestions to enhance email text editing and improve the likelihood of effective email responses. This research contributes to enhancing sender-receiver communication and customer engagement in email interactions.},
  file = {/Users/khawla/Zotero/storage/TBUFNW2S/Wang et al. - 2023 - PROMINET Prototype-based Multi-View Network for I.pdf}
}

@article{Wolf_2024_KeepFaithFaithful,
  title = {Keep the {{Faith}}: {{Faithful Explanations}} in {{Convolutional Neural Networks}} for {{Case-Based Reasoning}}},
  shorttitle = {Keep the {{Faith}}},
  author = {Wolf, Tom Nuno and Bongratz, Fabian and Rickmann, Anne-Marie and P{\"o}lsterl, Sebastian and Wachinger, Christian},
  year = {2024},
  month = mar,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {38},
  number = {6},
  pages = {5921--5929},
  issn = {2374-3468},
  doi = {10.1609/aaai.v38i6.28406},
  urldate = {2024-12-03},
  abstract = {Explaining predictions of black-box neural networks is crucial when applied to decision-critical tasks. Thus, attribution maps are commonly used to identify important image regions, despite prior work showing that humans prefer explanations based on similar examples. To this end, ProtoPNet learns a set of class-representative feature vectors (prototypes) for case-based reasoning. During inference, similarities of latent features to prototypes are linearly classified to form predictions and attribution maps are provided to explain the similarity. In this work, we evaluate whether architectures for case-based reasoning fulfill established axioms required for faithful explanations using the example of ProtoPNet. We show that such architectures allow the extraction of faithful explanations. However, we prove that the attribution maps used to explain the similarities violate the axioms. We propose a new procedure to extract explanations for trained ProtoPNets, named ProtoPFaith. Conceptually, these explanations are Shapley values, calculated on the similarity scores of each prototype. They allow to faithfully answer which prototypes are present in an unseen image and quantify each pixel's contribution to that presence, thereby complying with all axioms. The theoretical violations of ProtoPNet manifest in our experiments on three datasets (CUB-200-2011, Stanford Dogs, RSNA) and five architectures (ConvNet, ResNet, ResNet50, WideResNet50, ResNeXt50). Our experiments show a qualitative difference between the explanations given by ProtoPNet and ProtoPFaith. Additionally, we quantify the explanations with the Area Over the Perturbation Curve, on which ProtoPFaith outperforms ProtoPNet on all experiments by a factor {$>$}10{\textasciicircum}3.},
  copyright = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  keywords = {Explainable ML,Interpretable,ML: Transparent},
  file = {/Users/khawla/Zotero/storage/HPCYHD6C/Wolf et al. - 2024 - Keep the Faith Faithful Explanations in Convoluti.pdf}
}

@inproceedings{Xu_2024_TemporalPrototypeAwareLearning,
  title = {Temporal {{Prototype-Aware Learning}} for {{Active Voltage Control}} on {{Power Distribution Networks}}},
  booktitle = {Proceedings of the 30th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Xu, Feiyang and Liu, Shunyu and Qing, Yunpeng and Zhou, Yihe and Wang, Yuwen and Song, Mingli},
  year = {2024},
  month = aug,
  series = {{{KDD}} '24},
  pages = {3598--3609},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3637528.3671790},
  urldate = {2025-01-28},
  abstract = {Active Voltage Control (AVC) on the Power Distribution Networks (PDNs) aims to stabilize the voltage levels to ensure efficient and reliable operation of power systems. With the increasing integration of distributed energy resources, recent efforts have explored employing multi-agent reinforcement learning (MARL) techniques to realize effective AVC. Existing methods mainly focus on the acquisition of short-term AVC strategies, i.e., only learning AVC within the short-term training trajectories of a singular diurnal cycle. However, due to the dynamic nature of load demands and renewable energy, the operation states of real-world PDNs may exhibit significant distribution shifts across varying timescales (e.g., daily and seasonal changes). This can render those short-term strategies suboptimal or even obsolete when performing continuous AVC over extended periods. In this paper, we propose a novel temporal prototype-aware learning method, abbreviated as TPA, to learn time-adaptive AVC under short-term training trajectories. At the heart of TPA are two complementary components, namely multi-scale dynamic encoder and temporal prototype-aware policy, that can be readily incorporated into various MARL methods. The former component integrates a stacked transformer network to learn underlying temporal dependencies at different timescales of the PDNs, while the latter implements a learnable prototype matching mechanism to construct a dedicated AVC policy that can dynamically adapt to the evolving operation states. Experimental results on the AVC benchmark with different PDN sizes demonstrate that the proposed TPA surpasses the state-of-the-art counterparts not only in terms of control performance but also by offering model transferability. Our code is available at https://github.com/Canyizl/TPA-for-AVC.},
  isbn = {9798400704901},
  file = {/Users/khawla/Zotero/storage/AQD5U8VF/Xu et al. - 2024 - Temporal Prototype-Aware Learning for Active Volta.pdf}
}

@inproceedings{Xue_2024_ProtoPFormerConcentratingPrototypical,
  title = {{{ProtoPFormer}}: {{Concentrating}} on {{Prototypical Parts}} in {{Vision Transformers}} for {{Interpretable Image Recognition}}},
  shorttitle = {{{ProtoPFormer}}},
  booktitle = {Thirty-{{Third International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Xue, Mengqi and Huang, Qihan and Zhang, Haofei and Hu, Jingwen and Song, Jie and Song, Mingli and Jin, Canghong},
  year = {2024},
  month = aug,
  volume = {2},
  pages = {1516--1524},
  issn = {1045-0823},
  doi = {10.24963/ijcai.2024/168},
  urldate = {2024-12-03},
  abstract = {Electronic proceedings of IJCAI 2024},
  langid = {english},
  file = {/Users/khawla/Zotero/storage/8K2VAK68/Xue et al. - 2024 - ProtoPFormer Concentrating on Prototypical Parts .pdf}
}

@article{Zhang_2022_ProtGNNSelfExplainingGraph,
  title = {{{ProtGNN}}: {{Towards Self-Explaining Graph Neural Networks}}},
  shorttitle = {{{ProtGNN}}},
  author = {Zhang, Zaixi and Liu, Qi and Wang, Hao and Lu, Chengqiang and Lee, Cheekong},
  year = {2022},
  month = jun,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {36},
  number = {8},
  pages = {9127--9135},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v36i8.20898},
  urldate = {2024-12-03},
  abstract = {Despite the recent progress in Graph Neural Networks (GNNs), it remains challenging to explain the predictions made by GNNs. Existing explanation methods mainly focus on post-hoc explanations where another explanatory model is employed to provide explanations for a trained GNN. The fact that post-hoc methods fail to reveal the original reasoning process of GNNs raises the need of building GNNs with built-in interpretability. In this work, we propose Prototype Graph Neural Network (ProtGNN), which combines prototype learning with GNNs and provides a new perspective on the explanations of GNNs. In ProtGNN, the explanations are naturally derived from the case-based reasoning process and are actually used during classification. The prediction of ProtGNN is obtained by comparing the inputs to a few learned prototypes in the latent space. Furthermore, for better interpretability and higher efficiency, a novel conditional subgraph sampling module is incorporated to indicate which part of the input graph is most similar to each prototype in ProtGNN+. Finally, we evaluate our method on a wide range of datasets and perform concrete case studies. Extensive results show that ProtGNN and ProtGNN+ can provide inherent interpretability while achieving accuracy on par with the noninterpretable counterparts.},
  langid = {english},
  file = {/Users/khawla/Zotero/storage/Y5SQJ9TQ/Zhang et al. - 2022 - ProtGNN Towards Self-Explaining Graph Neural Netw.pdf}
}

@inproceedings{Zhang_2023_LearningSelectPrototypical,
  title = {Learning to Select Prototypical Parts for Interpretable Sequential Data Modeling},
  booktitle = {Proceedings of the {{Thirty-Seventh AAAI Conference}} on {{Artificial Intelligence}} and {{Thirty-Fifth Conference}} on {{Innovative Applications}} of {{Artificial Intelligence}} and {{Thirteenth Symposium}} on {{Educational Advances}} in {{Artificial Intelligence}}},
  author = {Zhang, Yifei and Gao, Neng and Ma, Cunqing},
  year = {2023},
  month = feb,
  series = {{{AAAI}}'23/{{IAAI}}'23/{{EAAI}}'23},
  volume = {37},
  pages = {6612--6620},
  publisher = {AAAI Press},
  doi = {10.1609/aaai.v37i5.25812},
  urldate = {2024-12-03},
  abstract = {Prototype-based interpretability methods provide intuitive explanations of model prediction by comparing samples to a reference set of memorized exemplars or typical representatives in terms of similarity. In the field of sequential data modeling, similarity calculations of prototypes are usually based on encoded representation vectors. However, due to highly recursive functions, there is usually a non-negligible disparity between the prototype-based explanations and the original input. In this work, we propose a Self-Explaining Selective Model (SESM) that uses a linear combination of prototypical concepts to explain its own predictions. The model employs the idea of case-based reasoning by selecting sub-sequences of the input that mostly activate different concepts as prototypical parts, which users can compare to sub-sequences selected from different example inputs to understand model decisions. For better interpretability, we design multiple constraints including diversity, stability, and locality as training objectives. Extensive experiments in different domains demonstrate that our method exhibits promising interpretability and competitive accuracy.},
  isbn = {978-1-57735-880-0},
  file = {/Users/khawla/Zotero/storage/V89GWBSH/Zhang et al. - 2023 - Learning to select prototypical parts for interpre.pdf}
}

@article{Ibrahim_2023_ExplainableConvNN, 
  author = {Ibrahim, Rami and Shafiq, M. Omair}, 
  title = {Explainable Convolutional Neural Networks: A Taxonomy, Review, and Future Directions}, 
  year = {2023}, 
  issue_date = {October 2023}, 
  publisher = {Association for Computing Machinery}, 
  address = {New York, NY, USA}, 
  volume = {55}, 
  number = {10}, 
  issn = {0360-0300}, 
  url = {https://doi.org/10.1145/3563691}, 
  doi = {10.1145/3563691}, 
  abstract = {Convolutional neural networks (CNNs) have shown promising results and have outperformed classical machine learning techniques in tasks such as image classification and object recognition. Their human-brain like structure enabled them to learn sophisticated features while passing images through their layers. However, their lack of explainability led to the demand for interpretations to justify their predictions. Research on Explainable AI or XAI has gained momentum to provide knowledge and insights into neural networks. This study summarizes the literature to gain more understanding of explainability in CNNs (i.e., Explainable Convolutional Neural Networks). We classify models that made efforts to improve the CNNs interpretation. We present and discuss taxonomies for XAI models that modify CNN architecture, simplify CNN representations, analyze feature relevance, and visualize interpretations. We review various metrics used to evaluate XAI interpretations. In addition, we discuss the applications and tasks of XAI models. This focused and extensive survey develops a perspective on this area by addressing suggestions for overcoming XAI interpretation challenges, like models’ generalization, unifying evaluation criteria, building robust models, and providing interpretations with semantic descriptions. Our taxonomy can be a reference to motivate future research in interpreting neural networks.}, 
  journal = {ACM Comput. Surv.}, 
  month = feb, 
  articleno = {206}, 
  numpages = {37}, 
  keywords = {Explainable AI, convolutional neural networks, Interpretable AI, survey} 
}

@article{Patricio_2024_ExplainableDeepLearning, 
  author = {Patr\'{\i}cio, Cristiano and Neves, Jo\~{a}o C. and Teixeira, Lu\'{\i}s F.}, 
  title = {Explainable Deep Learning Methods in Medical Image Classification: A Survey}, 
  year = {2023}, 
  issue_date = {April 2024}, 
  publisher = {Association for Computing Machinery}, 
  address = {New York, NY, USA}, 
  volume = {56}, 
  number = {4}, 
  issn = {0360-0300}, 
  url = {https://doi.org/10.1145/3625287}, 
  doi = {10.1145/3625287}, 
  abstract = {The remarkable success of deep learning has prompted interest in its application to medical imaging diagnosis. Even though state-of-the-art deep learning models have achieved human-level accuracy on the classification of different types of medical data, these models are hardly adopted in clinical workflows, mainly due to their lack of interpretability. The black-box nature of deep learning models has raised the need for devising strategies to explain the decision process of these models, leading to the creation of the topic of eXplainable Artificial Intelligence (XAI). In this context, we provide a thorough survey of XAI applied to medical imaging diagnosis, including visual, textual, example-based and concept-based explanation methods. Moreover, this work reviews the existing medical imaging datasets and the existing metrics for evaluating the quality of the explanations. In addition, we include a performance comparison among a set of report generation–based methods. Finally, the major challenges in applying XAI to medical imaging and the future research directions on the topic are discussed.}, 
  journal = {ACM Comput. Surv.}, 
  month = oct, 
  articleno = {85}, 
  numpages = {41}, 
  keywords = {medical image analysis, deep learning, interpretability, explainability, Explainable AI} 
}

@article{Nauta_2023_FromAnecdotalEvidence, 
  author = {Nauta, Meike and Trienes, Jan and Pathak, Shreyasi and Nguyen, Elisa and Peters, Michelle and Schmitt, Yasmin and Schl\"{o}tterer, J\"{o}rg and van Keulen, Maurice and Seifert, Christin}, 
  title = {From Anecdotal Evidence to Quantitative Evaluation Methods: A Systematic Review on Evaluating Explainable AI}, 
  year = {2023}, 
  issue_date = {December 2023}, 
  publisher = {Association for Computing Machinery}, 
  address = {New York, NY, USA}, 
  volume = {55}, number = {13s}, 
  issn = {0360-0300}, 
  url = {https://doi.org/10.1145/3583558}, 
  doi = {10.1145/3583558}, 
  abstract = {The rising popularity of explainable artificial intelligence (XAI) to understand high-performing black boxes raised the question of how to evaluate explanations of machine learning (ML) models. While interpretability and explainability are often presented as a subjectively validated binary property, we consider it a multi-faceted concept. We identify 12 conceptual properties, such as Compactness and Correctness, that should be evaluated for comprehensively assessing the quality of an explanation. Our so-called Co-12 properties serve as categorization scheme for systematically reviewing the evaluation practices of more than 300 papers published in the past 7 years at major AI and ML conferences that introduce an XAI method. We find that one in three papers evaluate exclusively with anecdotal evidence, and one in five papers evaluate with users. This survey also contributes to the call for objective, quantifiable evaluation methods by presenting an extensive overview of quantitative XAI evaluation methods. Our systematic collection of evaluation methods provides researchers and practitioners with concrete tools to thoroughly validate, benchmark, and compare new and existing XAI methods. The Co-12 categorization scheme and our identified evaluation methods open up opportunities to include quantitative metrics as optimization criteria during model training to optimize for accuracy and interpretability simultaneously.}, 
  journal = {ACM Comput. Surv.}, 
  month = jul, 
  articleno = {295}, 
  numpages = {42}, 
  keywords = {XAI, explainable AI, quantitative evaluation methods, interpretability, explainability, evaluation, interpretable machine learning, Explainable artificial intelligence} 
}

@inproceedings{Alpherts_2024_PerceptiveVisualUrban, 
  author = {Alpherts, Tim and Ghebreab, Sennay and Hsu, Yen-Chia and Van Noord, Nanne}, 
  title = {Perceptive Visual Urban Analytics is Not (Yet) Suitable for Municipalities}, 
  year = {2024}, 
  isbn = {9798400704505}, 
  publisher = {Association for Computing Machinery}, 
  address = {New York, NY, USA}, 
  url = {https://doi.org/10.1145/3630106.3658976}, 
  doi = {10.1145/3630106.3658976}, 
  abstract = {The use of Computer Vision, through a Perceptive Visual Urban Analytics (VUA) paradigm, has been proposed as a way for municipalities to more easily monitor their cities. However, prior studies fall short of actually investigating whether Perceptive VUA is ready for municipal use. In this paper we take a critical look at this paradigm by comparing key methods and evaluating them on usability and trustworthiness with municipal experts as well as Responsible AI and Computer Vision researchers. Based on on this evaluation we find that Perceptive VUA is not (yet) ready for municipal use as they do not incorporate domain knowledge and overly rely on spurious correlations. We conclude by providing recommendations for how to progress Perceptive VUA such that it may actually contribute to improving the liveability and quality of urban environments.}, booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency}, 
  pages = {1341–1354}, 
  numpages = {14}, 
  keywords = {Computer Vision, Explainability, Trustworthiness}, 
  location = {Rio de Janeiro, Brazil}, 
  series = {FAccT '24} 
}
@inproceedings{Zheng_2024_PrototypicalHashEncoding,
 author = {Zheng, Haiyang and Pu, Nan and Li, Wenjing and Sebe, Nicu and Zhong, Zhun},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {101428--101455},
 publisher = {Curran Associates, Inc.},
 title = {Prototypical Hash Encoding for On-the-Fly Fine-Grained Category Discovery},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/b78c69286221bbce7d12f98b5b45f337-Paper-Conference.pdf},
 volume = {37},
 year = {2024}
}
